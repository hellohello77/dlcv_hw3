# -*- coding: utf-8 -*-
"""hw3_2_eval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n9O-8_1XKNepgrdgAVoDcYokqsejoY5A
"""

# !wget -O vit_swin_large_30.ckpt https://www.dropbox.com/s/pze040c6vkgri97/vit_swin_large_30.ckpt?dl=1

import sys
from tqdm.notebook import tqdm


import os
import torch
from torchvision.datasets import CIFAR100
import json
import torchvision.transforms as T
from PIL import Image
from torch.utils.data import DataLoader
import csv
import PIL
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import torchvision
from torchvision import models
import torch.nn as nn
import numpy as np
import timm
import math
from tokenizers import Tokenizer
import argparse

# Load the model
device = "cuda" if torch.cuda.is_available() else "cpu"

parser = argparse.ArgumentParser()
parser.add_argument("img_file")
parser.add_argument("json_file")
args = parser.parse_args()

path_to_datafile = args.img_file
tokenizer = Tokenizer.from_file("./caption_tokenizer.json")

"""# Dataset"""

from torch.nn.utils.rnn import pad_sequence
class hw3_2_dataset:
    def __init__(self, filepath, transform):
        self.filepath = filepath
        self.file_list = [i for i in os.listdir(filepath)]
        self.transform = transform
    
    def __len__(self):
        return len(self.file_list)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.filepath, self.file_list[idx])
        img = Image.open(img_path).convert('RGB')
        transformed_img = self.transform(img)
        img.close()
        return transformed_img, self.file_list[idx]

transforms = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406],
               std=[0.229, 0.224, 0.225])
])

hw3_2_val = hw3_2_dataset(path_to_datafile, transforms)

val_loader = DataLoader(hw3_2_val, batch_size=4, shuffle=False)

"""# Model

## Positional Encoding
"""

class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, max_len: int = 5000):
        """
        param:
            d_model - Hidden dimensionality of the input.
            max_len - Maximum length of a sequence to expect.
        """
        super().__init__()

        self.d_model = d_model

        # create positional encoding
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() *
            (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)

        # not a parameter, but should be part of the modules state.
        self.register_buffer("pe", pe, persistent=False)

    def forward(self, x):
        x = x * math.sqrt(self.d_model)
        x = x + self.pe[:, :x.size(1)]
        return x

"""## Decoder Layer"""

from typing import Tuple
from torch import nn, Tensor
from torch.nn import MultiheadAttention


class DecoderLayer(nn.Module):

    def __init__(self, d_model: int, num_heads: int, feedforward_dim: int,
                 dropout: float):
        super(DecoderLayer, self).__init__()
        self.dec_self_attn = MultiheadAttention(d_model,
                                                num_heads,
                                                dropout=dropout)
        self.multihead_attn = MultiheadAttention(d_model,
                                                 num_heads,
                                                 dropout=dropout)

        self.self_attn_norm = nn.LayerNorm(d_model)
        self.multihead_norm = nn.LayerNorm(d_model)
        self.self_attn_dropout = nn.Dropout(dropout)
        self.multihead_dropout = nn.Dropout(dropout)

        self.ff = nn.Sequential(nn.Linear(d_model, feedforward_dim),
                                nn.ReLU(inplace=True), nn.Dropout(p=dropout),
                                nn.Linear(feedforward_dim, d_model))

        self.ff_norm = nn.LayerNorm(d_model)
        self.ff_dropout = nn.Dropout(dropout)

    def forward(self, dec_inputs: Tensor, enc_outputs: Tensor,
                tgt_mask: Tensor,
                tgt_pad_mask: Tensor) -> Tuple[Tensor, Tensor]:
        # self attention + resedual summation + norm
        output, _ = self.dec_self_attn(dec_inputs,
                                       dec_inputs,
                                       dec_inputs,
                                       attn_mask=tgt_mask,
                                       key_padding_mask=tgt_pad_mask)
        output = dec_inputs + self.self_attn_dropout(output)
        output = self.self_attn_norm(output)  # type: Tensor

        # # self attention + residual + norm + FF
        output2, attns = self.multihead_attn(output, enc_outputs, enc_outputs)
        output = output + self.multihead_dropout(output2)
        output = self.multihead_norm(output)

        output2 = self.ff(output)  # type: Tensor
        output = self.ff_norm(output + self.ff_dropout(output2))

        return output, attns

"""## Decoder & Transformer"""

from copy import deepcopy
from typing import Tuple

import torch
from torch import nn, Tensor

class Decoder(nn.Module):
    def __init__(self,
                 layer: DecoderLayer,
                 vocab_size: int,
                 d_model: int,
                 num_layers: int,
                 max_len: int,
                 dropout: float,
                 pad_id: int):
        super().__init__()

        self.pad_id = pad_id

        # Embedding layer + pos encoding
        self.cptn_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)
        self.pos_emb = PositionalEncoding(d_model, max_len)

        # Make copies of the decoder layer
        self.layers = nn.ModuleList(
            [deepcopy(layer) for _ in range(num_layers)])

        self.dropout = nn.Dropout(p=dropout)

    def get_attn_subsequent_mask(self, sz: int) -> Tensor:
        """
        Generates an upper-triangular matrix of -inf, with zeros on diag.
        """
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float(
            '-inf')).masked_fill(mask == 1, float(0.0))
        return mask
        # return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)

    def forward(self, tgt_cptn: Tensor,
                src_img: Tensor) -> Tuple[Tensor, Tensor]:
        # create masks, then pass to decoder
        tgt_pad_mask = (tgt_cptn == self.pad_id)
        tgt_mask = self.get_attn_subsequent_mask(tgt_cptn.size()[1])
        tgt_mask = tgt_mask.to(tgt_cptn.device)

        # encode captions + pos enc
        # (B, max_len) -> (B, max_len, d_model) -> (max_len, B, d_model)
        tgt_cptn = self.cptn_emb(tgt_cptn)  # type: Tensor
        tgt_cptn = self.dropout(self.pos_emb(tgt_cptn.permute(1, 0, 2)))

        attns_all = []
        for layer in self.layers:
            tgt_cptn, attns = layer(tgt_cptn, src_img, tgt_mask, tgt_pad_mask)
            attns_all.append(attns)
        # [layer_num, batch_size, head_num, max_len, encode_size**2]
        attns_all = torch.stack(attns_all)

        return tgt_cptn, attns_all


class Transformer(nn.Module):
    """
    """

    def __init__(self,
                 vocab_size: int,
                 d_model: int,
                 dec_ff_dim: int,
                 dec_n_layers: int,
                 dec_n_heads: int,
                 max_len: int,
                 dropout: float = 0.1,
                 pad_id: int = 0):
        super(Transformer, self).__init__()
        decoder_layer = DecoderLayer(d_model=d_model,
                                     num_heads=dec_n_heads,
                                     feedforward_dim=dec_ff_dim,
                                     dropout=dropout)
        self.encoder = timm.create_model('swin_large_patch4_window7_224', pretrained = False)
        self.decoder = Decoder(layer=decoder_layer,
                               vocab_size=vocab_size,
                               d_model=d_model,
                               num_layers=dec_n_layers,
                               max_len=max_len,
                               dropout=dropout,
                               pad_id=pad_id)

        self.predictor = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, images: Tensor,
                captions: Tensor) -> Tuple[Tensor, Tensor]:
        # encode, decode, predict
        images_encoded = self.encoder(images).unsqueeze(1).permute(1,0,2)  # type: Tensor
        tgt_cptn, attns = self.decoder(captions, images_encoded)
        predictions = self.predictor(tgt_cptn).permute(1,0,2)  # type: Tensor

        return predictions.contiguous(), attns.contiguous()

"""## Try"""

max_len = 75
cp_num = 18022

"""# Train

## Eval
"""

"""## Train"""

model = Transformer(cp_num, 1000, 2048, 8, 8, max_len, 0.1, 0)
model.load_state_dict(torch.load(f'./vit_swin_large_30.ckpt', map_location = device))
model = model.to(device)

from torch import Tensor

model.eval()
dictionary = dict()

for batch_idx, (imgs, file_names) in enumerate(val_loader):
      imgs: Tensor  # images [1, 3, 256, 256]
      imgs = imgs.to(device)
      start = torch.full(size=(imgs.size(0), 1),
                          fill_value=2,
                          dtype=torch.long,
                          device=device)
      with torch.no_grad():
          while start.size(1) <= (max_len - 2):
              logits, attns = model(imgs, start)
              logits: Tensor  # [k=1, 1, vsc]
              attns: Tensor  # [ln, k=1, hn, S=1, is]
              
              next_word_id = torch.max(logits, dim=2)[1][:,-1].unsqueeze(1)
              next_word_id = torch.as_tensor(next_word_id).to(device)
              jump = True
              for wo in range(next_word_id.size(0)):
                  if next_word_id[wo][0] != 0:
                      jump = False
                      break
              if jump:
                  break
              start = torch.cat(
                  (start, next_word_id), dim=1)
          start = start.cpu().tolist()
          
          for word in range(len(start)):
              word_string = tokenizer.decode(start[word], skip_special_tokens= True)
              name = os.path.splitext(file_names[word])[0]
              dictionary[name] = word_string
with open(args.json_file, "w") as outfile:
    json.dump(dictionary, outfile)